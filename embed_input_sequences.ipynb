{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b6973d",
   "metadata": {},
   "source": [
    "# Embed input DNA sequences\n",
    "\n",
    "In this notebook we extract the features (embeddings) of the input DNA sequences using the pretrained enformer.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d959be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import sonnet as snt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure the GPU is enabled\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "# path to the TF enformer model\n",
    "sys.path.append(\"../enformer/\")\n",
    "\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "from tensorflow.python.training import checkpoint_utils as cp\n",
    "from enformer import Enformer # not a package, but a module\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53189d4",
   "metadata": {},
   "source": [
    "## Loading the pretrained weights\n",
    "\n",
    "Since the saved TF Enformer model does not have a function to extract features (embeddings), we need to initiate our Enformer object defined in the file `enformer/enformer.py` (where we added an extra function to extract the embeddings) and load the weights from the pretrained model in the folder `weights/`. Unfortunately, this is not so straight forward, because the weights are named differently. This next code chunk takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3cae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function _yield_value at 0x0000016A7675EB80> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "WARNING: Entity <function _yield_value at 0x0000016A7675EB80> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "Correctly renamed 327 variables out of 329\n",
      "Duplicate names 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrik\\AppData\\Local\\Temp/ipykernel_26052/1432102018.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"transformed\"][ids] = tmp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables are updated with the correct values. Duplicates are considered!\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "# one simple input\n",
    "input1 = pd.read_csv(\"../data/dna_example_393_216.txt\", header=None).loc[0,0]\n",
    "\n",
    "# loading the pretrained model and initializing it with a prediction (probably not the best practice, but it works)\n",
    "enformer = Enformer()\n",
    "_ = enformer.predict_on_batch(tf.stop_gradient(one_hot_encode(input1)[np.newaxis]))\n",
    "\n",
    "# Updating the randomly initialized variables with the pretrained\n",
    "saved_variables = cp.list_variables(\"weights/variables/variables\")\n",
    "\n",
    "saved_names = [i[0] for i in saved_variables][1:-1]\n",
    "enformer_names = [i.name for i in enformer.variables]\n",
    "\n",
    "df = dict(originals = saved_names,\n",
    "          transformed = [rename_stored_variable(i) for i in saved_names]) # rename_stored_variable is a function from utils.py\n",
    "df = pd.DataFrame(df)\n",
    "df[\"check\"] = [str(i) in enformer_names for i in df.transformed]\n",
    "df[\"values\"] = [cp.load_variable(\"weights/variables/variables\", i) for i in df.originals]\n",
    "\n",
    "print(\"Correctly renamed \" + str(sum(df.check)) + \" variables out of \" + str(len(df.check)))\n",
    "print(\"Duplicate names\", len(df[\"transformed\"]) - len(df[\"transformed\"].unique() ))\n",
    "\n",
    "# Assumption: the duplicate variables maintain the same order in both lists\n",
    "for i in range(11):\n",
    "    xx = f'enformer/trunk/transformer/transformer/transformer_block_{i}/transformer_block_{i}/mlp/mlp/project_out/'\n",
    "    ids = [xx in j for j in df[\"transformed\"]]\n",
    "    tmp = df[\"transformed\"][ids].to_list()\n",
    "    xx2 = f'enformer/trunk/transformer/transformer/transformer_block_{i}/transformer_block_{i}/mlp/mlp/project_in/'\n",
    "    tmp[:2] = [j.replace(xx, xx2) for j in tmp[:2]]\n",
    "    df[\"transformed\"][ids] = tmp   \n",
    "    \n",
    "df2 = df.copy()\n",
    "for v in enformer.variables:\n",
    "    a = int(df2[\"transformed\"][df2[\"transformed\"] == v.name].index.values[0])\n",
    "    v.assign(df2[\"values\"][a])\n",
    "    df2.loc[a,:] = None\n",
    "print(\"All variables are updated with the correct values. Duplicates are considered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd655e26",
   "metadata": {},
   "source": [
    "## Extracting and storing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbc88a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features(embeddings) ...: 100%|████████████████████████████████████████| 6149/6149 [44:40<00:00,  2.29seq/s]\n"
     ]
    }
   ],
   "source": [
    "#TODO!\n",
    "seq_dir = \"/data/DNA_seqs/\"\n",
    "seq_files= os.listdir(seq_dir)\n",
    "feat_dir = \"/data/DNA_feats\"\n",
    "\n",
    "with tqdm(total = len(seq_files), desc = \"Extracting features(embeddings) ...\", unit = 'seq') as prog_bar:\n",
    "    for i, seq_name in enumerate(seq_files):\n",
    "        \n",
    "        #TODO!\n",
    "        #Read sequence\n",
    "        seq = pd.read_csv(seq_dir+seq_name)\n",
    "\n",
    "        name = seq_dir + seq_name\n",
    "        feat_file = seq_name.split(\".\")[0] + \".npy\"\n",
    "        \n",
    "        feat = enformer.extract_features(tf.stop_gradient(one_hot_encode(seq)[np.newaxis]))\n",
    "        feat = feat.numpy()\n",
    "\n",
    "        np.save(feat_dir + feat_file, feat)\n",
    "        prog_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42270e4",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Checking if the model works correctly...\n",
    "\n",
    "To be sure that the features are correct and we did not messed up we have to checked 2 things:\n",
    "- If the \"adapted\" model still works well on the original task\n",
    "- If the \"adapted\" model gets the same (or quite similar) output as the original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d907c",
   "metadata": {},
   "source": [
    "## The original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "class Enformer_Original:\n",
    "\n",
    "  def __init__(self, tfhub_url):\n",
    "    self._model = hub.load(tfhub_url).model\n",
    "\n",
    "  def predict_on_batch(self, inputs):\n",
    "    predictions = self._model.predict_on_batch(inputs)\n",
    "    return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "  @tf.function\n",
    "  def contribution_input_grad(self, input_sequence,\n",
    "                              target_mask, output_head='human'):\n",
    "    input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "    target_mask_mass = tf.reduce_sum(target_mask)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(input_sequence)\n",
    "      prediction = tf.reduce_sum(\n",
    "          target_mask[tf.newaxis] *\n",
    "          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "\n",
    "    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "    input_grad = tf.squeeze(input_grad, axis=0)\n",
    "    return tf.reduce_sum(input_grad, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4eff8",
   "metadata": {},
   "source": [
    "### Performance on the original task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbda16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, head, max_steps=None):\n",
    "  metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "  @tf.function\n",
    "  def predict(x):\n",
    "    return model(x, is_training=False)\n",
    "\n",
    "  for i, batch in tqdm(enumerate(dataset)):\n",
    "    if max_steps is not None and i > max_steps:\n",
    "      break\n",
    "    metric.update_state(batch['target'], predict(batch['sequence']))\n",
    "\n",
    "  return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f62c5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:57,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'PearsonR': 0.7012999}\n"
     ]
    }
   ],
   "source": [
    "# need to download the data first...\n",
    "seq_id = {i.split(\"_\")[1] for i in os.listdir(\"data/test_data/target/\")}\n",
    "tar_id = {i.split(\"_\")[1] for i in os.listdir(\"data/test_data/sequence/\")}\n",
    "\n",
    "ds = []\n",
    "for i in seq_id.intersection(tar_id):\n",
    "    ds = ds + [{'target': tf.convert_to_tensor(np.load(\"data/test_data/target\"+\"tar_\"+ i)),\n",
    "                'sequence': tf.convert_to_tensor(np.load(\"data/test_data/sequence/\"+\"seq_\"+ i))\n",
    "               }]\n",
    "               \n",
    "metrics_mouse = evaluate_model(enformer,\n",
    "                               dataset= ds,\n",
    "                               head='mouse',\n",
    "                               max_steps=100)\n",
    "print('')\n",
    "print({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb5176",
   "metadata": {},
   "source": [
    "### Comparison between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c15ee624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "#TODO: select an input sequence\n",
    "# seq = pd.read_csv(\"../data/\").loc[0,0]\n",
    "x = tf.stop_gradient(one_hot_encode(seq)[np.newaxis])\n",
    "enformer_hub = Enformer_Original('https://tfhub.dev/deepmind/enformer/1')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    my_pred = enformer.predict_on_batch(x)\n",
    "    pred_hub = enformer_hub.predict_on_batch(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a40a6198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14715277"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_pred \n",
    "np.mean((pred_hub[\"mouse\"] - my_pred )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0d1e073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99503214],\n",
       "       [0.99503214, 1.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(tf.reshape(my_pred, (896*1643)), pred_hub[\"mouse\"].reshape((896*1643)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6dc42cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 896, 1643), dtype=float32, numpy=\n",
       "array([[[0.4128465 , 0.608387  , 1.6581078 , ..., 0.7503728 ,\n",
       "         1.8233689 , 1.6437178 ],\n",
       "        [0.8164966 , 1.2658758 , 2.6417766 , ..., 2.0881827 ,\n",
       "         7.3151054 , 4.889168  ],\n",
       "        [1.7602785 , 2.9756105 , 4.58759   , ..., 1.7699678 ,\n",
       "         3.6342628 , 3.8983936 ],\n",
       "        ...,\n",
       "        [0.05717006, 0.10506745, 0.08914189, ..., 0.308557  ,\n",
       "         0.4925663 , 0.517363  ],\n",
       "        [0.06500754, 0.08711809, 0.08176833, ..., 0.4391661 ,\n",
       "         1.1487088 , 0.7453046 ],\n",
       "        [0.04310616, 0.06987476, 0.05819134, ..., 0.25947037,\n",
       "         0.5015786 , 0.4432723 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pred "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
